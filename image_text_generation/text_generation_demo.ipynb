{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tz362/anaconda3/envs/pandagpt/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/tz362/anaconda3/envs/pandagpt/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/tz362/anaconda3/envs/pandagpt/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Initializing visual encoder from ../pretrained_ckpt/imagebind_ckpt ...\n",
      "Visual encoder initialized.\n",
      "Initializing language decoder from ../pretrained_ckpt/vicuna_ckpt/converted/vicuna_full ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9f8800a54a43709d27ace83ad649f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33554432 || all params: 6771978240 || trainable%: 0.49548936530546206\n",
      "Language decoder initialized.\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from model.openllama import OpenLLAMAPEFTModel\n",
    "\n",
    "# Initialize the model\n",
    "args = {\n",
    "    'model': 'openllama_peft',\n",
    "    'imagebind_ckpt_path': '../pretrained_ckpt/imagebind_ckpt',\n",
    "    'vicuna_ckpt_path': '../pretrained_ckpt/vicuna_ckpt/converted/vicuna_full',\n",
    "    'delta_ckpt_path': '../pretrained_ckpt/pandagpt_ckpt/7b/pandagpt_7b_max_len_1024/pytorch_model.pt',\n",
    "    'stage': 2,\n",
    "    'max_tgt_len': 128,\n",
    "    'lora_r': 32,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.1,\n",
    "}\n",
    "print(\"Initializing model...\")\n",
    "model = OpenLLAMAPEFTModel(**args)\n",
    "delta_ckpt = torch.load(args['delta_ckpt_path'], map_location=torch.device('cpu'))\n",
    "model.load_state_dict(delta_ckpt, strict=False)\n",
    "model = model.eval().half().cuda()\n",
    "print(\"Model initialized.\")\n",
    "\n",
    "def generate_response(prompt_text, image_path=None, audio_path=None, video_path=None, thermal_path=None, top_p=0.01, temperature=1.0, max_length=128):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    response = model.generate({\n",
    "        'prompt': prompt_text,\n",
    "        'image_paths': [image_path] if image_path else [],\n",
    "        'audio_paths': [audio_path] if audio_path else [],\n",
    "        'video_paths': [video_path] if video_path else [],\n",
    "        'thermal_paths': [thermal_path] if thermal_path else [],\n",
    "        'top_p': top_p,\n",
    "        'temperature': temperature,\n",
    "        'max_tgt_len': max_length,\n",
    "        'modality_embeds': []\n",
    "\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The image shows a man standing in a field, holding a gun in his hand. He is wearing a white shirt and black pants, and appears to be pointing the gun towards the camera. The man's face is obscured by the gun, but it is clear that he is holding it with a serious expression. The background of the image is a grassy field with some trees visible in the distance.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt_text = \"Can you describe the image?\"\n",
    "image_path = \"/home/tz362/Desktop/usenix_artifact_eval/adversarial_illusions/outputs/assets/street.png\"  # Update this path to your image\n",
    "\n",
    "# Generate response\n",
    "response = generate_response(prompt_text, image_path=image_path)\n",
    "print(\"Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandagpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
