{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcfdf64-a4ec-436b-9486-d091b7042213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import imagebind.data as data\n",
    "from IPython.display import Audio\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353780d-6675-4d80-9cdf-2f8142ce9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c29e5-d216-4f6a-9b6b-81358df561c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_mel_bins = 128\n",
    "num_frames = 204\n",
    "sample_rate = 16000\n",
    "n_fft = 400\n",
    "hop_length = n_fft//4\n",
    "win_length = n_fft\n",
    "\n",
    "# Function to create a Mel inversion matrix\n",
    "def create_mel_inversion_matrix(sr, n_fft, n_mels, fmin=0.0, fmax=None):\n",
    "    # Create a Mel filter bank using torchaudio\n",
    "    mel_fb = T.MelScale(n_mels, sr, f_min=fmin, f_max=fmax, n_stft=n_fft//2+1, norm=None)\n",
    "    # Convert the filter bank to a tensor\n",
    "    mel_fb_tensor = torch.tensor(mel_fb.fb, dtype=torch.float)\n",
    "    # Calculate the pseudo inverse\n",
    "    inversion_matrix = torch.pinverse(mel_fb_tensor)\n",
    "    print(inversion_matrix.shape)\n",
    "    \n",
    "    return inversion_matrix\n",
    "\n",
    "def inverse_it(mel_spectrogram):\n",
    "    \n",
    "    # Create the Mel inversion matrix\n",
    "    inversion_matrix = create_mel_inversion_matrix(sample_rate, n_fft, num_mel_bins)\n",
    "\n",
    "    # Invert the Mel spectrogram to a power spectrogram\n",
    "    power_spectrogram = torch.matmul(mel_spectrogram, inversion_matrix)\n",
    "\n",
    "    # Create an InverseMelScale transform\n",
    "    inverse_mel_scale_transform = T.InverseMelScale(\n",
    "        n_stft=n_fft//2+1,\n",
    "        n_mels=num_mel_bins,\n",
    "        sample_rate=sample_rate,\n",
    "        f_min=0.0,\n",
    "        f_max=sample_rate//2,\n",
    "        norm=None\n",
    "    )\n",
    "\n",
    "    # Apply the InverseMelScale transform to the Mel spectrogram\n",
    "    spectrogram = inverse_mel_scale_transform(mel_spectrogram.T)\n",
    "\n",
    "    # Initialize Griffin-Lim transform\n",
    "    griffin_lim = T.GriffinLim(n_fft=n_fft, n_iter=32, win_length=win_length, hop_length=hop_length)\n",
    "\n",
    "    # Recover the waveform from the spectrogram\n",
    "    recovered_waveform = griffin_lim(spectrogram)\n",
    "    \n",
    "    Audio(recovered_waveform, rate=16000)\n",
    "    \n",
    "    return recovered_waveform\n",
    "\n",
    "def inverse_normalize(melspec, mean=-4.268, std=9.138):\n",
    "    return melspec * std + mean\n",
    "\n",
    "\n",
    "def combine_results(audio):\n",
    "    results = list()\n",
    "    for i in range(audio.shape[1]):\n",
    "        res = inverse_it(inverse_normalize(audio.clone().detach().cpu().float()[0][i][0]).T)\n",
    "        results.append(res)\n",
    "    # return [results[0], results[1], results[2]]\n",
    "    return [results[0][:-5000], results[1][:-5000], results[2][:-2000]]\n",
    "    \n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def get_results(audio_tensor):\n",
    "    mel_spectrogram_np = audio_tensor[0, :, 0, :, :198].detach().clone().cpu().float()\n",
    "    audio_cropped = torch.cat(tuple(mel_spectrogram_np), dim=-1)\n",
    "    return inverse_it(inverse_normalize(audio_cropped.T))\n",
    "\n",
    "\n",
    "def convert_mp3_to_wav(input_path, output_path, bitrate, duration, shift=0):\n",
    "    # Load the MP3 file\n",
    "    audio = AudioSegment.from_mp3(input_path)\n",
    "    print(len(audio))\n",
    "    # Set the desired duration\n",
    "    audio = audio[shift:duration * 1000 + shift]\n",
    "\n",
    "    # Set the desired bitrate\n",
    "    audio = audio.set_frame_rate(bitrate)\n",
    "\n",
    "    # Export the audio as a WAV file\n",
    "    audio.export(output_path, format='wav')\n",
    "    return audio\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68112c34-4dd8-4796-9e03-d593c751efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loader(path):\n",
    "    image_tensor = data.load_and_transform_vision_data([path], 'cpu')\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc36da8",
   "metadata": {},
   "source": [
    "## You need access to the ImageNet validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17561da6-e0f0-40f2-a63d-4abdb2415afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageNet('./data/imagenet/', split='val', loader=custom_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de955c7d-c053-4c64-81aa-f1bc3c3d02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "test_transform = transforms.Compose([\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(224),\n",
    "            transforms.Resize(132),\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "            \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9f387-dcba-455a-a9b7-fc9a8a2fef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnorm = UnNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092b190-7909-43cd-b22f-9d47e2c50274",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5544c-fdaf-4d45-a8ef-5ecd5717a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76718069-81c1-4588-8ae1-2b32ef8a716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeds = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a542939",
   "metadata": {},
   "source": [
    "### Embed all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e182c2-9151-4855-b859-e7535296c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(50000)):\n",
    "    image_tensor, _ = dataset[i]\n",
    "    with torch.no_grad():\n",
    "        embed = model({'vision': image_tensor.to(device)})\n",
    "        all_embeds.append(embed['vision'].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502158b9-2c82-403e-8e56-333b698b1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "catted_embeds = torch.cat(all_embeds, dim=0)\n",
    "torch.save(catted_embeds, 'embeds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cbecc-f1e6-421f-8dcc-b3faa174d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "catted_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb77c3-3f95-41ab-a8bd-3e685c9a193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim =torch.zeros([50000, 50000], dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442b1f7-142f-4d7d-9af8-3fea0025d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e628c4-9b72-468e-b4e7-1992e07c53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = catted_embeds[:10]\n",
    "z = catted_embeds[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49353b3-234f-4d7d-886c-d4558b72c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(dataset.classes):\n",
    "    for entry in x:\n",
    "        if 'sheep' in entry:\n",
    "            print(i, x)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbff471-43ba-4027-b0f2-0b3e047b9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(dataset.classes):\n",
    "    for entry in x:\n",
    "        if 'sheep' in entry:\n",
    "            print(i, x)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a91d74-66c5-49ce-9355-380618b798f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, (lbl) in enumerate(dataset.targets):\n",
    "    if lbl == 348:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5fe04a-6d49-4b59-b1fa-a93cc944ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes[348]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca90f24-d788-4780-93c3-a6d7fc391fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embeds = torch.zeros( [len(dataset.classes), 1024])\n",
    "test_embeds = torch.zeros( [5000, 1024])\n",
    "test_labels = torch.ones( [5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284db32-203d-4f54-942a-84d1a28bbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "catted_embeds[ 50*(i+1)-2: 50*(i+1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1e147-75fd-4b0b-a8ab-7b21ebae3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset.classes)):\n",
    "    mean_embeds[i] = torch.mean(catted_embeds[ 50*i: 50*(i+1)-5] , dim=0)\n",
    "    test_embeds[5*i: 5*(i+1)] = catted_embeds[ 50*(i+1) - 5 : 50*(i+1)]\n",
    "    test_labels[5*i: 5*(i+1)] = i\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9b115-e3b0-4195-a17a-82a3d968eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes[270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc024e3-df83-46af-827a-f32b20e33e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes[17439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466a638-a130-4db6-9e14-bd9d43c86a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text_list=[\"Everything we see hides another thing, we always want to see what is hidden by what we see, but it is impossible.\",]\n",
    "# image_paths=[\".assets/car_image.jpg\"] #\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "# image_paths = ['horse.jpg']\n",
    "audio_paths=[\"all_assets/wolves.wav\"] #\"all_assets/police3.wav\"] # \".assets/car_audio.wav\", \".assets/bird_audio.wav\"\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    # ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    # ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # embeddings = imagebind(inputs)\n",
    "    # text_embed = model.forward( {ModalityType.TEXT: data.load_and_transform_text(text_list, device)}, normalize=False)[ModalityType.TEXT] \n",
    "    audio_embed = model.forward( {ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device)}, normalize=False)[ModalityType.AUDIO]\n",
    "    # image_embed = model.forward({ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device)}, normalize=False)[ModalityType.VISION]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1e3b7-31a2-4da8-b147-97c5966823a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./all_assets/wolves.wav\"\n",
    "orig_waveform, sr = torchaudio.load(path)\n",
    "print(sr)\n",
    "audio_tensor = data.load_and_transform_audio_data([path], 'cpu', )\n",
    "# Audio(torch.cat(combine_results(audio_tensor)), rate=16000)\n",
    "# Audio(orig_waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f98d3-aead-49b0-a8dc-ce3d5bcd5f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.0001 * torch.rand_like(audio_tensor).to(device)\n",
    "X.requires_grad_(True)\n",
    "audio_tensor = audio_tensor.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d83b2-ec3c-47d1-97a4-d6c90b91836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "optimizer = optim.SGD([X], lr=0.005)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                              T_max = epochs, # Maximum number of iterations.\n",
    "                              eta_min = 1e-5) # Minimum learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03591b-8925-4ec7-908e-a32ba071f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(epochs))\n",
    "saved_dict = dict()\n",
    "\n",
    "for i in pbar:\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    embeds = model.forward({'audio': X + audio_tensor}, normalize=True)\n",
    "\n",
    "    loss = 1 - F.cosine_similarity(embeds['audio'], ideal_embed, dim=1).mean()\n",
    "    grads = torch.autograd.grad(outputs=loss, inputs=X)\n",
    "\n",
    "    \n",
    "    X = X - lr * grads[0].sign()\n",
    "    X.detach().clamp_(min=-0.05, max=0.05)\n",
    "\n",
    "    \n",
    "    pbar.set_postfix({'loss': loss.item(), 'lr': lr, 'norm': X.detach().norm().item(), 'saved': list(saved_dict.keys())})\n",
    "    \n",
    "        \n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    del grads, embeds, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e7d1b-01c4-45d8-935d-082838b0ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.forward({'audio': (audio_tensor + X).detach()}, normalize=True)['audio'].detach().cpu() @ catted_embeds.T).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210b3fa-2cb6-4841-8b12-a6055f0abf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.forward({'audio': audio_tensor.to(device)}, normalize=True)['audio'].detach().cpu() @ mean_embeds.T).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb9999-e19c-409e-92ad-9187e2744b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e2d87-b0ed-4c2c-b36b-499a0c14dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423606a0-be16-4def-9050-9b97ae348b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram_np = audio_tensor.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "# Split the 3 channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a13497-ab49-433e-b021-a9f73bfd4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram_np = audio_tensor[0, :, 0, :, :198].detach().cpu()\n",
    "channel_1, channel_2, channel_3 = mel_spectrogram_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff016277-32ca-4491-af4c-0544ad731ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05697d39-627e-4658-be19-7bfd9e3640b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each of the 3 mel-spectrograms\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.imshow(torch.cat(tuple(mel_spectrogram_np), dim=-1), aspect='auto', origin='lower', cmap='viridis')\n",
    "ax.set_title(f'Mel-frequency spectrogram')\n",
    "ax.set_ylabel('Mel bands')\n",
    "# ax[i].colorbar()\n",
    "\n",
    "ax.set_xlabel('Time frames')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47847e-da96-4944-818f-8d51bb3244ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Audio(get_results(audio_tensor), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03772cfe-e240-4338-95cb-16a22ff4649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(get_results(X), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15e74c-f6e8-460e-a759-6ae3721ea948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each of the 3 mel-spectrograms\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "    ax[i].imshow(mel_spectrogram_np[i, 0], aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax[i].set_title(f'Mel-frequency spectrogram - Channel {i+1}')\n",
    "    ax[i].set_ylabel('Mel bands')\n",
    "    # ax[i].colorbar()\n",
    "\n",
    "ax[-1].set_xlabel('Time frames')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ac061-ef35-4a91-9071-1ac2701cce09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imagebind]",
   "language": "python",
   "name": "conda-env-imagebind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
