{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import imagebind.data as data\n",
    "from IPython.display import Audio\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0abe3c-0d3c-4bd4-9abc-01db05b549aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ee68c-75fd-42b0-871b-9ed6b8b65846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "unnorm = UnNormalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "def di(image):\n",
    "    display(transform(unnorm(image.data.detach().cpu())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0f287-bc0c-446c-a163-cf079951a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "t = transforms.Normalize(\n",
    "                    mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a306ca-f1f0-48b6-a649-1db227988321",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d40e1f-63bd-4441-af62-d91fa966cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e248571-8c29-4d5c-9fb7-ec552333bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagebind = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09918fa0-567e-411f-9d03-68b6096cb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_list = ['Everything we see hides another thing, we always want to see what is hidden by what we see, but it is impossible.']\n",
    "# image_paths=[\".assets/car_image.jpg\"] #\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "# image_paths = ['horse.jpg']\n",
    "# audio_paths=[\"all_assets/police3.wav\"] # \".assets/car_audio.wav\", \".assets/bird_audio.wav\"\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    # ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    # ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # embeddings = imagebind(inputs)\n",
    "    text_embed = model.forward( {ModalityType.TEXT: data.load_and_transform_text(text_list, device)}, normalize=False)[ModalityType.TEXT] \n",
    "    # audio_embed = model.forward( {ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device)}, normalize=False)[ModalityType.AUDIO]\n",
    "    # image_embed = model.forward({ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device)}, normalize=False)[ModalityType.VISION]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b524cbd-3899-4e89-bad1-94eb66b447fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_embed.cpu(), 'hack/embed_eye.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311cb8e-fb68-4452-83b1-f83221dfdfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8a566-143f-4816-aa46-26bad1542466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_list=[\"a classical concert\",]\n",
    "# image_paths=[\"all_assets/dove.jpg\"] #\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "# image_paths = ['horse.jpg']\n",
    "# audio_paths=[\".assets/chips.wav\"] # \".assets/car_audio.wav\", \".assets/bird_audio.wav\"\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    # ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    # ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # embeddings = imagebind(inputs)\n",
    "    text_embed = model.forward( {ModalityType.TEXT: data.load_and_transform_text(text_list, device)}, normalize=False)[ModalityType.TEXT] \n",
    "    # audio_embed = model.forward( {ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device)}, normalize=False)[ModalityType.AUDIO]\n",
    "    # image_embed = model.forward({ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device)}, normalize=False)[ModalityType.VISION]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ce2fc-1fd4-49b2-a8d6-dae74579618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = data.load_and_transform_vision_data(['all_assets/dog_image.jpg'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffcec8-a82e-4fc7-96fe-a91ad26739ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(transform(unnorm(image_tensor.data[0].detach().cpu())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16787dfc-99d2-402f-8f6b-dfd96aacac2e",
   "metadata": {},
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a7007-8b9d-4601-9bd2-d38c4c2ebc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.01 * torch.rand_like(image_tensor)\n",
    "X.requires_grad_(True)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62f003-2c63-412d-883e-bbad193af611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 8000\n",
    "optimizer = optim.SGD([X], lr=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                              T_max = epochs, # Maximum number of iterations.\n",
    "                              eta_min = 1e-5) # Minimum learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817c0b1-0d35-4279-8a49-63c102791186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "\n",
    "\n",
    "for i in pbar:\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    sum_tensor = (image_tensor + X)\n",
    "    # sum_tensor.detach().clamp_(min=image_tensor.min(), max=image_tensor.max())\n",
    "    \n",
    "    embeds = model.forward({'vision': sum_tensor}, normalize=True)\n",
    "    loss = 1 - F.cosine_similarity(embeds['vision'], ideal_embed, dim=1).mean()\n",
    "    \n",
    "    \n",
    "    res3 = torch.autograd.grad(outputs=loss, inputs=X)\n",
    "    \n",
    "    X = X - lr * res3[0].sign()\n",
    "    X.detach().clamp_(min=-0.05, max=0.05)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    pbar.set_postfix({'loss': loss.item(), 'lr': lr, 'norm': X.detach().norm().item()})\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    del res3, embeds, loss\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a98db7-68e2-4415-b5b9-6ffc7b818c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(transform(unnorm((X+image_tensor).data[0].detach().cpu())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da071086-b056-439d-8731-88aa257ff258",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((X+image_tensor).detach().cpu(), './hack/image_attack.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imagebind]",
   "language": "python",
   "name": "conda-env-imagebind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
