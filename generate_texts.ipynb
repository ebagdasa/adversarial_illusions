{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfbcb49-56dd-4e73-8f2a-cf819c1f93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3f87be-9202-4ce8-b6c8-4e8e16fff1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ae3cb2-65e1-4028-8e94-0bec229eba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/miniconda3/envs/imagebind/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/home/eugene/miniconda3/envs/imagebind/lib/python3.8/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/eugene/miniconda3/envs/imagebind/lib/python3.8/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model.openllama import OpenLLAMAPEFTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f4d30-144c-4fa0-9c3f-2fdef4b751e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "from model.ImageBind.models import imagebind_model\n",
    "from model.ImageBind.models.imagebind_model import ModalityType\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "imagebind = imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind = imagebind[0]\n",
    "imagebind.eval()\n",
    "imagebind.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae209c3-a077-4979-8e7f-b6845633cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ce2dd1-dd7f-4181-bbbd-8e75448f1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'openllama_peft',\n",
    "    'imagebind_ckpt_path': './models/converted/imagebind/',\n",
    "    'vicuna_ckpt_path': './models/converted/vicuna_full/',\n",
    "    'delta_ckpt_path': './models/converted/pandagpt_7b_max_len_1024/pytorch_model.pt',\n",
    "    'stage': 2,\n",
    "    'max_tgt_len': 128,\n",
    "    'lora_r': 32,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fe532",
   "metadata": {},
   "source": [
    "**We load ImageBind on GPU and PandaGPT on CPU** Please decide based\n",
    "on your GPU size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4391ca44-4baa-40ed-89bd-cadc59e7e3e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing visual encoder from /home/eugene/models/converted/imagebind/ ...\n",
      "Visual encoder initialized.\n",
      "Initializing language decoder from /home/eugene/models/converted/vicuna_full/ ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4b11e3ec0f4571bb6abc654a66529e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33554432 || all params: 6771978240 || trainable%: 0.49548936530546206\n",
      "Language decoder initialized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['visual_encoder.modality_preprocessors.vision.cls_token', 'visual_encoder.modality_preprocessors.vision.rgbt_stem.proj.1.weight', 'visual_encoder.modality_preprocessors.vision.pos_embedding_helper.pos_embed', 'visual_encoder.modality_preprocessors.text.pos_embed', 'visual_encoder.modality_preprocessors.text.mask', 'visual_encoder.modality_preprocessors.text.token_embedding.weight', 'visual_encoder.modality_preprocessors.audio.cls_token', 'visual_encoder.modality_preprocessors.audio.rgbt_stem.proj.weight', 'visual_encoder.modality_preprocessors.audio.rgbt_stem.norm_layer.weight', 'visual_encoder.modality_preprocessors.audio.rgbt_stem.norm_layer.bias', 'visual_encoder.modality_preprocessors.audio.pos_embedding_helper.pos_embed', 'visual_encoder.modality_preprocessors.depth.cls_token', 'visual_encoder.modality_preprocessors.depth.depth_stem.proj.weight', 'visual_encoder.modality_preprocessors.depth.depth_stem.norm_layer.weight', 'visual_encoder.modality_preprocessors.depth.depth_stem.norm_layer.bias', 'visual_encoder.modality_preprocessors.depth.pos_embedding_helper.pos_embed', 'visual_encoder.modality_preprocessors.thermal.cls_token', 'visual_encoder.modality_preprocessors.thermal.rgbt_stem.proj.weight', 'visual_encoder.modality_preprocessors.thermal.rgbt_stem.norm_layer.weight', 'visual_encoder.modality_preprocessors.thermal.rgbt_stem.norm_layer.bias', 'visual_encoder.modality_preprocessors.thermal.pos_embedding_helper.pos_embed', 'visual_encoder.modality_preprocessors.imu.pos_embed', 'visual_encoder.modality_preprocessors.imu.cls_token', 'visual_encoder.modality_preprocessors.imu.imu_stem.proj.weight', 'visual_encoder.modality_preprocessors.imu.imu_stem.norm_layer.weight', 'visual_encoder.modality_preprocessors.imu.imu_stem.norm_layer.bias', 'visual_encoder.modality_trunks.vision.pre_transformer_layer.0.weight', 'visual_encoder.modality_trunks.vision.pre_transformer_layer.0.bias', 'visual_encoder.modality_trunks.vision.blocks.0.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.0.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.0.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.0.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.0.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.0.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.0.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.0.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.0.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.0.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.0.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.0.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.1.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.1.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.1.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.1.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.1.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.1.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.1.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.1.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.1.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.1.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.1.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.1.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.2.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.2.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.2.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.2.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.2.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.2.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.2.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.2.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.2.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.2.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.2.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.2.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.3.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.3.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.3.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.3.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.3.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.3.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.3.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.3.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.3.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.3.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.3.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.3.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.4.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.4.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.4.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.4.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.4.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.4.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.4.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.4.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.4.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.4.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.4.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.4.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.5.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.5.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.5.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.5.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.5.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.5.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.5.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.5.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.5.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.5.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.5.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.5.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.6.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.6.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.6.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.6.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.6.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.6.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.6.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.6.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.6.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.6.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.6.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.6.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.7.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.7.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.7.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.7.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.7.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.7.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.7.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.7.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.7.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.7.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.7.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.7.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.8.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.8.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.8.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.8.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.8.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.8.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.8.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.8.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.8.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.8.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.8.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.8.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.9.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.9.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.9.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.9.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.9.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.9.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.9.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.9.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.9.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.9.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.9.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.9.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.10.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.10.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.10.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.10.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.10.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.10.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.10.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.10.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.10.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.10.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.10.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.10.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.11.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.11.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.11.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.11.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.11.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.11.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.11.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.11.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.11.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.11.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.11.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.11.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.12.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.12.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.12.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.12.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.12.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.12.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.12.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.12.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.12.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.12.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.12.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.12.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.13.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.13.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.13.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.13.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.13.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.13.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.13.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.13.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.13.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.13.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.13.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.13.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.14.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.14.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.14.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.14.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.14.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.14.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.14.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.14.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.14.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.14.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.14.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.14.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.15.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.15.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.15.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.15.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.15.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.15.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.15.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.15.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.15.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.15.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.15.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.15.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.16.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.16.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.16.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.16.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.16.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.16.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.16.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.16.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.16.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.16.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.16.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.16.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.17.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.17.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.17.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.17.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.17.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.17.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.17.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.17.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.17.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.17.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.17.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.17.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.18.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.18.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.18.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.18.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.18.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.18.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.18.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.18.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.18.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.18.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.18.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.18.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.19.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.19.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.19.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.19.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.19.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.19.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.19.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.19.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.19.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.19.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.19.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.19.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.20.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.20.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.20.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.20.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.20.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.20.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.20.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.20.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.20.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.20.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.20.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.20.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.21.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.21.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.21.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.21.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.21.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.21.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.21.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.21.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.21.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.21.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.21.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.21.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.22.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.22.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.22.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.22.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.22.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.22.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.22.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.22.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.22.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.22.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.22.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.22.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.23.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.23.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.23.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.23.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.23.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.23.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.23.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.23.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.23.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.23.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.23.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.23.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.24.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.24.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.24.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.24.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.24.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.24.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.24.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.24.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.24.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.24.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.24.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.24.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.25.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.25.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.25.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.25.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.25.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.25.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.25.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.25.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.25.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.25.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.25.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.25.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.26.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.26.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.26.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.26.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.26.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.26.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.26.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.26.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.26.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.26.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.26.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.26.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.27.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.27.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.27.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.27.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.27.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.27.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.27.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.27.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.27.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.27.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.27.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.27.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.28.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.28.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.28.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.28.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.28.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.28.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.28.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.28.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.28.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.28.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.28.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.28.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.29.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.29.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.29.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.29.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.29.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.29.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.29.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.29.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.29.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.29.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.29.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.29.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.30.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.30.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.30.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.30.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.30.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.30.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.30.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.30.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.30.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.30.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.30.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.30.norm_2.bias', 'visual_encoder.modality_trunks.vision.blocks.31.attn.in_proj_weight', 'visual_encoder.modality_trunks.vision.blocks.31.attn.in_proj_bias', 'visual_encoder.modality_trunks.vision.blocks.31.attn.out_proj.weight', 'visual_encoder.modality_trunks.vision.blocks.31.attn.out_proj.bias', 'visual_encoder.modality_trunks.vision.blocks.31.norm_1.weight', 'visual_encoder.modality_trunks.vision.blocks.31.norm_1.bias', 'visual_encoder.modality_trunks.vision.blocks.31.mlp.fc1.weight', 'visual_encoder.modality_trunks.vision.blocks.31.mlp.fc1.bias', 'visual_encoder.modality_trunks.vision.blocks.31.mlp.fc2.weight', 'visual_encoder.modality_trunks.vision.blocks.31.mlp.fc2.bias', 'visual_encoder.modality_trunks.vision.blocks.31.norm_2.weight', 'visual_encoder.modality_trunks.vision.blocks.31.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.0.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.0.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.0.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.0.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.0.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.0.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.0.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.0.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.0.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.0.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.0.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.0.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.1.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.1.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.1.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.1.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.1.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.1.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.1.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.1.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.1.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.1.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.1.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.1.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.2.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.2.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.2.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.2.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.2.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.2.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.2.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.2.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.2.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.2.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.2.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.2.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.3.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.3.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.3.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.3.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.3.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.3.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.3.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.3.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.3.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.3.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.3.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.3.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.4.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.4.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.4.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.4.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.4.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.4.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.4.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.4.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.4.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.4.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.4.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.4.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.5.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.5.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.5.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.5.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.5.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.5.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.5.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.5.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.5.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.5.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.5.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.5.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.6.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.6.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.6.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.6.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.6.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.6.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.6.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.6.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.6.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.6.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.6.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.6.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.7.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.7.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.7.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.7.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.7.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.7.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.7.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.7.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.7.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.7.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.7.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.7.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.8.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.8.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.8.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.8.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.8.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.8.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.8.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.8.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.8.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.8.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.8.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.8.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.9.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.9.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.9.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.9.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.9.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.9.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.9.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.9.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.9.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.9.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.9.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.9.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.10.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.10.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.10.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.10.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.10.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.10.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.10.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.10.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.10.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.10.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.10.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.10.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.11.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.11.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.11.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.11.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.11.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.11.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.11.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.11.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.11.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.11.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.11.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.11.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.12.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.12.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.12.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.12.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.12.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.12.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.12.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.12.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.12.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.12.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.12.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.12.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.13.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.13.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.13.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.13.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.13.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.13.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.13.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.13.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.13.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.13.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.13.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.13.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.14.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.14.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.14.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.14.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.14.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.14.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.14.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.14.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.14.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.14.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.14.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.14.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.15.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.15.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.15.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.15.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.15.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.15.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.15.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.15.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.15.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.15.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.15.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.15.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.16.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.16.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.16.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.16.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.16.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.16.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.16.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.16.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.16.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.16.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.16.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.16.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.17.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.17.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.17.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.17.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.17.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.17.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.17.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.17.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.17.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.17.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.17.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.17.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.18.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.18.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.18.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.18.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.18.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.18.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.18.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.18.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.18.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.18.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.18.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.18.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.19.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.19.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.19.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.19.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.19.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.19.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.19.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.19.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.19.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.19.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.19.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.19.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.20.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.20.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.20.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.20.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.20.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.20.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.20.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.20.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.20.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.20.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.20.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.20.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.21.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.21.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.21.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.21.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.21.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.21.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.21.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.21.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.21.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.21.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.21.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.21.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.22.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.22.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.22.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.22.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.22.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.22.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.22.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.22.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.22.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.22.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.22.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.22.norm_2.bias', 'visual_encoder.modality_trunks.text.blocks.23.attn.in_proj_weight', 'visual_encoder.modality_trunks.text.blocks.23.attn.in_proj_bias', 'visual_encoder.modality_trunks.text.blocks.23.attn.out_proj.weight', 'visual_encoder.modality_trunks.text.blocks.23.attn.out_proj.bias', 'visual_encoder.modality_trunks.text.blocks.23.norm_1.weight', 'visual_encoder.modality_trunks.text.blocks.23.norm_1.bias', 'visual_encoder.modality_trunks.text.blocks.23.mlp.fc1.weight', 'visual_encoder.modality_trunks.text.blocks.23.mlp.fc1.bias', 'visual_encoder.modality_trunks.text.blocks.23.mlp.fc2.weight', 'visual_encoder.modality_trunks.text.blocks.23.mlp.fc2.bias', 'visual_encoder.modality_trunks.text.blocks.23.norm_2.weight', 'visual_encoder.modality_trunks.text.blocks.23.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.0.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.0.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.0.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.0.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.0.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.0.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.0.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.0.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.0.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.0.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.0.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.0.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.0.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.0.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.1.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.1.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.1.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.1.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.1.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.1.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.1.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.1.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.1.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.1.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.1.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.1.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.1.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.1.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.2.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.2.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.2.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.2.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.2.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.2.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.2.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.2.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.2.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.2.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.2.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.2.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.2.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.2.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.3.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.3.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.3.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.3.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.3.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.3.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.3.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.3.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.3.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.3.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.3.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.3.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.3.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.3.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.4.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.4.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.4.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.4.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.4.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.4.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.4.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.4.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.4.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.4.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.4.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.4.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.4.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.4.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.5.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.5.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.5.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.5.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.5.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.5.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.5.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.5.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.5.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.5.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.5.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.5.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.5.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.5.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.6.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.6.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.6.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.6.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.6.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.6.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.6.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.6.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.6.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.6.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.6.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.6.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.6.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.6.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.7.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.7.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.7.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.7.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.7.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.7.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.7.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.7.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.7.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.7.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.7.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.7.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.7.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.7.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.8.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.8.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.8.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.8.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.8.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.8.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.8.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.8.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.8.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.8.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.8.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.8.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.8.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.8.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.9.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.9.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.9.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.9.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.9.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.9.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.9.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.9.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.9.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.9.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.9.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.9.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.9.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.9.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.10.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.10.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.10.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.10.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.10.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.10.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.10.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.10.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.10.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.10.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.10.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.10.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.10.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.10.norm_2.bias', 'visual_encoder.modality_trunks.audio.blocks.11.attn.in_proj_weight', 'visual_encoder.modality_trunks.audio.blocks.11.attn.in_proj_bias', 'visual_encoder.modality_trunks.audio.blocks.11.attn.bias_k', 'visual_encoder.modality_trunks.audio.blocks.11.attn.bias_v', 'visual_encoder.modality_trunks.audio.blocks.11.attn.out_proj.weight', 'visual_encoder.modality_trunks.audio.blocks.11.attn.out_proj.bias', 'visual_encoder.modality_trunks.audio.blocks.11.norm_1.weight', 'visual_encoder.modality_trunks.audio.blocks.11.norm_1.bias', 'visual_encoder.modality_trunks.audio.blocks.11.mlp.fc1.weight', 'visual_encoder.modality_trunks.audio.blocks.11.mlp.fc1.bias', 'visual_encoder.modality_trunks.audio.blocks.11.mlp.fc2.weight', 'visual_encoder.modality_trunks.audio.blocks.11.mlp.fc2.bias', 'visual_encoder.modality_trunks.audio.blocks.11.norm_2.weight', 'visual_encoder.modality_trunks.audio.blocks.11.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.0.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.0.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.0.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.0.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.0.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.0.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.0.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.0.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.0.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.0.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.0.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.0.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.0.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.0.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.1.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.1.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.1.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.1.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.1.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.1.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.1.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.1.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.1.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.1.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.1.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.1.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.1.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.1.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.2.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.2.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.2.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.2.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.2.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.2.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.2.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.2.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.2.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.2.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.2.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.2.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.2.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.2.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.3.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.3.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.3.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.3.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.3.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.3.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.3.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.3.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.3.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.3.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.3.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.3.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.3.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.3.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.4.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.4.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.4.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.4.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.4.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.4.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.4.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.4.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.4.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.4.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.4.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.4.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.4.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.4.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.5.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.5.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.5.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.5.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.5.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.5.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.5.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.5.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.5.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.5.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.5.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.5.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.5.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.5.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.6.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.6.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.6.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.6.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.6.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.6.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.6.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.6.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.6.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.6.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.6.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.6.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.6.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.6.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.7.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.7.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.7.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.7.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.7.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.7.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.7.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.7.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.7.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.7.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.7.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.7.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.7.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.7.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.8.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.8.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.8.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.8.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.8.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.8.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.8.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.8.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.8.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.8.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.8.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.8.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.8.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.8.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.9.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.9.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.9.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.9.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.9.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.9.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.9.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.9.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.9.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.9.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.9.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.9.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.9.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.9.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.10.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.10.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.10.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.10.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.10.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.10.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.10.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.10.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.10.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.10.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.10.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.10.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.10.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.10.norm_2.bias', 'visual_encoder.modality_trunks.depth.blocks.11.attn.in_proj_weight', 'visual_encoder.modality_trunks.depth.blocks.11.attn.in_proj_bias', 'visual_encoder.modality_trunks.depth.blocks.11.attn.bias_k', 'visual_encoder.modality_trunks.depth.blocks.11.attn.bias_v', 'visual_encoder.modality_trunks.depth.blocks.11.attn.out_proj.weight', 'visual_encoder.modality_trunks.depth.blocks.11.attn.out_proj.bias', 'visual_encoder.modality_trunks.depth.blocks.11.norm_1.weight', 'visual_encoder.modality_trunks.depth.blocks.11.norm_1.bias', 'visual_encoder.modality_trunks.depth.blocks.11.mlp.fc1.weight', 'visual_encoder.modality_trunks.depth.blocks.11.mlp.fc1.bias', 'visual_encoder.modality_trunks.depth.blocks.11.mlp.fc2.weight', 'visual_encoder.modality_trunks.depth.blocks.11.mlp.fc2.bias', 'visual_encoder.modality_trunks.depth.blocks.11.norm_2.weight', 'visual_encoder.modality_trunks.depth.blocks.11.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.0.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.0.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.0.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.0.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.0.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.0.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.0.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.0.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.0.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.0.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.0.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.0.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.0.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.0.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.1.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.1.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.1.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.1.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.1.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.1.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.1.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.1.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.1.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.1.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.1.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.1.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.1.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.1.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.2.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.2.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.2.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.2.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.2.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.2.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.2.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.2.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.2.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.2.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.2.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.2.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.2.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.2.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.3.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.3.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.3.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.3.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.3.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.3.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.3.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.3.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.3.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.3.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.3.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.3.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.3.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.3.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.4.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.4.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.4.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.4.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.4.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.4.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.4.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.4.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.4.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.4.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.4.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.4.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.4.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.4.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.5.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.5.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.5.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.5.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.5.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.5.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.5.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.5.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.5.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.5.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.5.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.5.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.5.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.5.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.6.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.6.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.6.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.6.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.6.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.6.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.6.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.6.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.6.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.6.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.6.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.6.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.6.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.6.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.7.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.7.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.7.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.7.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.7.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.7.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.7.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.7.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.7.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.7.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.7.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.7.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.7.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.7.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.8.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.8.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.8.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.8.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.8.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.8.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.8.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.8.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.8.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.8.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.8.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.8.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.8.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.8.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.9.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.9.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.9.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.9.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.9.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.9.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.9.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.9.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.9.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.9.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.9.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.9.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.9.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.9.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.10.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.10.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.10.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.10.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.10.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.10.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.10.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.10.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.10.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.10.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.10.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.10.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.10.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.10.norm_2.bias', 'visual_encoder.modality_trunks.thermal.blocks.11.attn.in_proj_weight', 'visual_encoder.modality_trunks.thermal.blocks.11.attn.in_proj_bias', 'visual_encoder.modality_trunks.thermal.blocks.11.attn.bias_k', 'visual_encoder.modality_trunks.thermal.blocks.11.attn.bias_v', 'visual_encoder.modality_trunks.thermal.blocks.11.attn.out_proj.weight', 'visual_encoder.modality_trunks.thermal.blocks.11.attn.out_proj.bias', 'visual_encoder.modality_trunks.thermal.blocks.11.norm_1.weight', 'visual_encoder.modality_trunks.thermal.blocks.11.norm_1.bias', 'visual_encoder.modality_trunks.thermal.blocks.11.mlp.fc1.weight', 'visual_encoder.modality_trunks.thermal.blocks.11.mlp.fc1.bias', 'visual_encoder.modality_trunks.thermal.blocks.11.mlp.fc2.weight', 'visual_encoder.modality_trunks.thermal.blocks.11.mlp.fc2.bias', 'visual_encoder.modality_trunks.thermal.blocks.11.norm_2.weight', 'visual_encoder.modality_trunks.thermal.blocks.11.norm_2.bias', 'visual_encoder.modality_trunks.imu.blocks.0.attn.in_proj_weight', 'visual_encoder.modality_trunks.imu.blocks.0.attn.in_proj_bias', 'visual_encoder.modality_trunks.imu.blocks.0.attn.bias_k', 'visual_encoder.modality_trunks.imu.blocks.0.attn.bias_v', 'visual_encoder.modality_trunks.imu.blocks.0.attn.out_proj.weight', 'visual_encoder.modality_trunks.imu.blocks.0.attn.out_proj.bias', 'visual_encoder.modality_trunks.imu.blocks.0.norm_1.weight', 'visual_encoder.modality_trunks.imu.blocks.0.norm_1.bias', 'visual_encoder.modality_trunks.imu.blocks.0.mlp.fc1.weight', 'visual_encoder.modality_trunks.imu.blocks.0.mlp.fc1.bias', 'visual_encoder.modality_trunks.imu.blocks.0.mlp.fc2.weight', 'visual_encoder.modality_trunks.imu.blocks.0.mlp.fc2.bias', 'visual_encoder.modality_trunks.imu.blocks.0.norm_2.weight', 'visual_encoder.modality_trunks.imu.blocks.0.norm_2.bias', 'visual_encoder.modality_trunks.imu.blocks.1.attn.in_proj_weight', 'visual_encoder.modality_trunks.imu.blocks.1.attn.in_proj_bias', 'visual_encoder.modality_trunks.imu.blocks.1.attn.bias_k', 'visual_encoder.modality_trunks.imu.blocks.1.attn.bias_v', 'visual_encoder.modality_trunks.imu.blocks.1.attn.out_proj.weight', 'visual_encoder.modality_trunks.imu.blocks.1.attn.out_proj.bias', 'visual_encoder.modality_trunks.imu.blocks.1.norm_1.weight', 'visual_encoder.modality_trunks.imu.blocks.1.norm_1.bias', 'visual_encoder.modality_trunks.imu.blocks.1.mlp.fc1.weight', 'visual_encoder.modality_trunks.imu.blocks.1.mlp.fc1.bias', 'visual_encoder.modality_trunks.imu.blocks.1.mlp.fc2.weight', 'visual_encoder.modality_trunks.imu.blocks.1.mlp.fc2.bias', 'visual_encoder.modality_trunks.imu.blocks.1.norm_2.weight', 'visual_encoder.modality_trunks.imu.blocks.1.norm_2.bias', 'visual_encoder.modality_trunks.imu.blocks.2.attn.in_proj_weight', 'visual_encoder.modality_trunks.imu.blocks.2.attn.in_proj_bias', 'visual_encoder.modality_trunks.imu.blocks.2.attn.bias_k', 'visual_encoder.modality_trunks.imu.blocks.2.attn.bias_v', 'visual_encoder.modality_trunks.imu.blocks.2.attn.out_proj.weight', 'visual_encoder.modality_trunks.imu.blocks.2.attn.out_proj.bias', 'visual_encoder.modality_trunks.imu.blocks.2.norm_1.weight', 'visual_encoder.modality_trunks.imu.blocks.2.norm_1.bias', 'visual_encoder.modality_trunks.imu.blocks.2.mlp.fc1.weight', 'visual_encoder.modality_trunks.imu.blocks.2.mlp.fc1.bias', 'visual_encoder.modality_trunks.imu.blocks.2.mlp.fc2.weight', 'visual_encoder.modality_trunks.imu.blocks.2.mlp.fc2.bias', 'visual_encoder.modality_trunks.imu.blocks.2.norm_2.weight', 'visual_encoder.modality_trunks.imu.blocks.2.norm_2.bias', 'visual_encoder.modality_trunks.imu.blocks.3.attn.in_proj_weight', 'visual_encoder.modality_trunks.imu.blocks.3.attn.in_proj_bias', 'visual_encoder.modality_trunks.imu.blocks.3.attn.bias_k', 'visual_encoder.modality_trunks.imu.blocks.3.attn.bias_v', 'visual_encoder.modality_trunks.imu.blocks.3.attn.out_proj.weight', 'visual_encoder.modality_trunks.imu.blocks.3.attn.out_proj.bias', 'visual_encoder.modality_trunks.imu.blocks.3.norm_1.weight', 'visual_encoder.modality_trunks.imu.blocks.3.norm_1.bias', 'visual_encoder.modality_trunks.imu.blocks.3.mlp.fc1.weight', 'visual_encoder.modality_trunks.imu.blocks.3.mlp.fc1.bias', 'visual_encoder.modality_trunks.imu.blocks.3.mlp.fc2.weight', 'visual_encoder.modality_trunks.imu.blocks.3.mlp.fc2.bias', 'visual_encoder.modality_trunks.imu.blocks.3.norm_2.weight', 'visual_encoder.modality_trunks.imu.blocks.3.norm_2.bias', 'visual_encoder.modality_trunks.imu.blocks.4.attn.in_proj_weight', 'visual_encoder.modality_trunks.imu.blocks.4.attn.in_proj_bias', 'visual_encoder.modality_trunks.imu.blocks.4.attn.bias_k', 'visual_encoder.modality_trunks.imu.blocks.4.attn.bias_v', 'visual_encoder.modality_trunks.imu.blocks.4.attn.out_proj.weight', 'visual_encoder.modality_trunks.imu.blocks.4.attn.out_proj.bias', 'visual_encoder.modality_trunks.imu.blocks.4.norm_1.weight', 'visual_encoder.modality_trunks.imu.blocks.4.norm_1.bias', 'visual_encoder.modality_trunks.imu.blocks.4.mlp.fc1.weight', 'visual_encoder.modality_trunks.imu.blocks.4.mlp.fc1.bias', 'visual_encoder.modality_trunks.imu.blocks.4.mlp.fc2.weight', 'visual_encoder.modality_trunks.imu.blocks.4.mlp.fc2.bias', 'visual_encoder.modality_trunks.imu.blocks.4.norm_2.weight', 'visual_encoder.modality_trunks.imu.blocks.4.norm_2.bias', 'visual_encoder.modality_trunks.imu.blocks.5.attn.in_proj_weight', 'visual_encoder.modality_trunks.imu.blocks.5.attn.in_proj_bias', 'visual_encoder.modality_trunks.imu.blocks.5.attn.bias_k', 'visual_encoder.modality_trunks.imu.blocks.5.attn.bias_v', 'visual_encoder.modality_trunks.imu.blocks.5.attn.out_proj.weight', 'visual_encoder.modality_trunks.imu.blocks.5.attn.out_proj.bias', 'visual_encoder.modality_trunks.imu.blocks.5.norm_1.weight', 'visual_encoder.modality_trunks.imu.blocks.5.norm_1.bias', 'visual_encoder.modality_trunks.imu.blocks.5.mlp.fc1.weight', 'visual_encoder.modality_trunks.imu.blocks.5.mlp.fc1.bias', 'visual_encoder.modality_trunks.imu.blocks.5.mlp.fc2.weight', 'visual_encoder.modality_trunks.imu.blocks.5.mlp.fc2.bias', 'visual_encoder.modality_trunks.imu.blocks.5.norm_2.weight', 'visual_encoder.modality_trunks.imu.blocks.5.norm_2.bias', 'visual_encoder.modality_heads.vision.0.weight', 'visual_encoder.modality_heads.vision.0.bias', 'visual_encoder.modality_heads.vision.2.weight', 'visual_encoder.modality_heads.text.proj.0.weight', 'visual_encoder.modality_heads.text.proj.0.bias', 'visual_encoder.modality_heads.text.proj.1.weight', 'visual_encoder.modality_heads.audio.0.weight', 'visual_encoder.modality_heads.audio.0.bias', 'visual_encoder.modality_heads.audio.2.weight', 'visual_encoder.modality_heads.depth.0.weight', 'visual_encoder.modality_heads.depth.0.bias', 'visual_encoder.modality_heads.depth.2.weight', 'visual_encoder.modality_heads.thermal.0.weight', 'visual_encoder.modality_heads.thermal.0.bias', 'visual_encoder.modality_heads.thermal.2.weight', 'visual_encoder.modality_heads.imu.0.weight', 'visual_encoder.modality_heads.imu.0.bias', 'visual_encoder.modality_heads.imu.3.weight', 'visual_encoder.modality_postprocessors.text.1.log_logit_scale', 'visual_encoder.modality_postprocessors.audio.1.log_logit_scale', 'visual_encoder.modality_postprocessors.depth.1.log_logit_scale', 'visual_encoder.modality_postprocessors.thermal.1.log_logit_scale', 'visual_encoder.modality_postprocessors.imu.1.log_logit_scale', 'llama_model.base_model.model.model.embed_tokens.weight', 'llama_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.0.input_layernorm.weight', 'llama_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.1.input_layernorm.weight', 'llama_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.2.input_layernorm.weight', 'llama_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.3.input_layernorm.weight', 'llama_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.4.input_layernorm.weight', 'llama_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.5.input_layernorm.weight', 'llama_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.6.input_layernorm.weight', 'llama_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.7.input_layernorm.weight', 'llama_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.8.input_layernorm.weight', 'llama_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.9.input_layernorm.weight', 'llama_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.10.input_layernorm.weight', 'llama_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.11.input_layernorm.weight', 'llama_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.12.input_layernorm.weight', 'llama_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.13.input_layernorm.weight', 'llama_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.14.input_layernorm.weight', 'llama_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.15.input_layernorm.weight', 'llama_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.16.input_layernorm.weight', 'llama_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.17.input_layernorm.weight', 'llama_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.18.input_layernorm.weight', 'llama_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.19.input_layernorm.weight', 'llama_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.20.input_layernorm.weight', 'llama_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.21.input_layernorm.weight', 'llama_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.22.input_layernorm.weight', 'llama_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.23.input_layernorm.weight', 'llama_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.24.input_layernorm.weight', 'llama_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.25.input_layernorm.weight', 'llama_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.26.input_layernorm.weight', 'llama_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.27.input_layernorm.weight', 'llama_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.28.input_layernorm.weight', 'llama_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.29.input_layernorm.weight', 'llama_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.30.input_layernorm.weight', 'llama_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.31.input_layernorm.weight', 'llama_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'llama_model.base_model.model.model.norm.weight', 'llama_model.base_model.model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OpenLLAMAPEFTModel(**args)\n",
    "delta_ckpt = torch.load(args['delta_ckpt_path'], map_location=torch.device('cpu'))\n",
    "model.load_state_dict(delta_ckpt, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f7ca247e-9e0b-4271-a586-4a8c56909ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(path, audio=True):\n",
    "    modality_cache=[]\n",
    "    if audio:\n",
    "        prompt_text = 'What is this sound?'\n",
    "        audio_path = path\n",
    "        image_path = None\n",
    "    else:\n",
    "        prompt_text = 'What is on this image?'\n",
    "        audio_path = None\n",
    "        image_path = path\n",
    "    \n",
    "    response = model.generate({\n",
    "            'prompt': prompt_text,\n",
    "            'image_paths': [image_path] if image_path else [],\n",
    "            'audio_paths': [audio_path] if audio_path else [],\n",
    "            'video_paths': [video_path] if video_path else [],\n",
    "            'thermal_paths': [thermal_path] if thermal_path else [],\n",
    "            'top_p': top_p,\n",
    "            'temperature': temperature,\n",
    "            'max_tgt_len': max_length,\n",
    "            'modality_embeds': modality_cache\n",
    "        })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "eb8e3cc7-0698-494b-b50e-1051073326f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b2411437-79b7-4070-9f88-6185cac79c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT!! OVERWRITING AUDIO LOADING WITH MODIFIED INPUT: /home/eugene/ImageBind/hack/audio_wolves_to_sheep.pt\n",
      "feature_embeds:  torch.Size([1, 1, 4096])\n",
      "p_after_embeds torch.Size([1, 15, 4096])\n",
      "inputs_embeds torch.Size([1, 23, 4096])\n",
      "input_embeds torch.Size([1, 23, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sound of a sheep bleating can be described as a soft, low-pitched, and continuous noise. It is a common'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(path='/home/eugene/ImageBind/hack/audio_wolves_to_sheep.pt', audio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4eafc343-6764-45b6-937d-181f5e652b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT!! OVERWRITING AUDIO LOADING WITH MODIFIED INPUT: /home/eugene/ImageBind/hack/audio_wolves_to_sheep.pt\n",
      "feature_embeds:  torch.Size([1, 1, 4096])\n",
      "p_after_embeds torch.Size([1, 14, 4096])\n",
      "inputs_embeds torch.Size([1, 22, 4096])\n",
      "input_embeds torch.Size([1, 22, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sound in the image is a sheep bleating, which is a common vocalization made by sheep. The sheep is standing on a grassy hillside, and its bleating can be heard in the background. This sound is a natural part of the communication and social behavior of sheep, and it can indicate various emotions or needs, such as seeking attention, expressing discomfort, or calling for companionship among other sheep.'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(path='/home/eugene/ImageBind/hack/audio_wolves_to_sheep.pt', audio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a57b34-6b85-4f87-be30-e4d47ebe4caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "fabfb52e-d2c9-4548-a046-c778268c699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_embeds:  torch.Size([1, 1, 4096])\n",
      "p_after_embeds torch.Size([1, 14, 4096])\n",
      "inputs_embeds torch.Size([1, 22, 4096])\n",
      "input_embeds torch.Size([1, 22, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sound in the image is a rattling noise coming from a bicycle wheel. The wheel is attached to the bike and appears to be loose or not properly secured, causing the rattling noise. This could be due to a worn-out or damaged wheel, a loose spoke, or an improperly tightened wheel nut. The rattling noise might be concerning for the bicycle owner, as it could indicate a potential safety issue or a need for maintenance to prevent further damage to the wheel or the bike itself.'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(path='/home/eugene/ImageBind/all_assets/chains.wav', audio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "99217e34-52f8-42d0-80b8-fd6e02524437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_embeds:  torch.Size([1, 1, 4096])\n",
      "p_after_embeds torch.Size([1, 14, 4096])\n",
      "inputs_embeds torch.Size([1, 22, 4096])\n",
      "input_embeds torch.Size([1, 22, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sound is a dog barking.'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(path='/home/eugene/ImageBind/all_assets/wolves.wav', audio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1ddd1fd9-e14a-451b-a55c-e69791f35e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_embeds:  torch.Size([1, 1, 4096])\n",
      "p_after_embeds torch.Size([1, 14, 4096])\n",
      "inputs_embeds torch.Size([1, 22, 4096])\n",
      "input_embeds torch.Size([1, 22, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sound is a dog barking.'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(path='/home/eugene/ImageBind/all_assets/wolves.wav', audio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371dea6-1d95-4f59-bb90-326a0b65dd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f6e98f4a-f67b-4178-b58b-b3b4261e8d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_embeds:  torch.Size([1, 1, 4096])\n",
      "p_after_embeds torch.Size([1, 15, 4096])\n",
      "inputs_embeds torch.Size([1, 23, 4096])\n",
      "input_embeds torch.Size([1, 23, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The image shows a white wolf standing on its hind legs with its front paws resting on its chest. The wolf's ears are perked up and its eyes are focused on something in the distance. The wolf's mouth is slightly open, and its tongue is hanging out. The wolf appears to be alert and attentive, possibly reacting to a sound or smell in the environment.\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(path='/home/eugene/ImageBind/all_assets/wolves.wav', audio=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imagebind]",
   "language": "python",
   "name": "conda-env-imagebind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
