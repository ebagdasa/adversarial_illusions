{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfbcb49-56dd-4e73-8f2a-cf819c1f93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3f87be-9202-4ce8-b6c8-4e8e16fff1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae3cb2-65e1-4028-8e94-0bec229eba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.openllama import OpenLLAMAPEFTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f4d30-144c-4fa0-9c3f-2fdef4b751e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data\n",
    "import torch\n",
    "from model.ImageBind.models import imagebind_model\n",
    "from model.ImageBind.models.imagebind_model import ModalityType\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "imagebind = imagebind_model.imagebind_huge(pretrained=True)\n",
    "imagebind = imagebind[0]\n",
    "imagebind.eval()\n",
    "imagebind.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae209c3-a077-4979-8e7f-b6845633cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ce2dd1-dd7f-4181-bbbd-8e75448f1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'openllama_peft',\n",
    "    'imagebind_ckpt_path': './models/converted/imagebind/',\n",
    "    'vicuna_ckpt_path': './models/converted/vicuna_full/',\n",
    "    'delta_ckpt_path': './models/converted/pandagpt_7b_max_len_1024/pytorch_model.pt',\n",
    "    'stage': 2,\n",
    "    'max_tgt_len': 128,\n",
    "    'lora_r': 32,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fe532",
   "metadata": {},
   "source": [
    "**We load ImageBind on GPU and PandaGPT on CPU** Please decide based\n",
    "on your GPU size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391ca44-4baa-40ed-89bd-cadc59e7e3e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = OpenLLAMAPEFTModel(**args)\n",
    "delta_ckpt = torch.load(args['delta_ckpt_path'], map_location=torch.device('cpu'))\n",
    "model.load_state_dict(delta_ckpt, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f7ca247e-9e0b-4271-a586-4a8c56909ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(path, audio=True):\n",
    "    modality_cache=[]\n",
    "    if audio:\n",
    "        prompt_text = 'What is this sound?'\n",
    "        audio_path = path\n",
    "        image_path = None\n",
    "    else:\n",
    "        prompt_text = 'What is on this image?'\n",
    "        audio_path = None\n",
    "        image_path = path\n",
    "    \n",
    "    response = model.generate({\n",
    "            'prompt': prompt_text,\n",
    "            'image_paths': [image_path] if image_path else [],\n",
    "            'audio_paths': [audio_path] if audio_path else [],\n",
    "            'video_paths': [video_path] if video_path else [],\n",
    "            'thermal_paths': [thermal_path] if thermal_path else [],\n",
    "            'top_p': top_p,\n",
    "            'temperature': temperature,\n",
    "            'max_tgt_len': max_length,\n",
    "            'modality_embeds': modality_cache\n",
    "        })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "eb8e3cc7-0698-494b-b50e-1051073326f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b2411437-79b7-4070-9f88-6185cac79c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT!! OVERWRITING AUDIO LOADING WITH MODIFIED INPUT: /home/eugene/ImageBind/hack/audio_wolves_to_sheep.pt\n",
      "feature_embeds:  torch.Size([1, 1, 4096])\n",
      "p_after_embeds torch.Size([1, 15, 4096])\n",
      "inputs_embeds torch.Size([1, 23, 4096])\n",
      "input_embeds torch.Size([1, 23, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sound of a sheep bleating can be described as a soft, low-pitched, and continuous noise. It is a common'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(path='ImageBind/hack/audio_wolves_to_sheep.pt', audio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f6e98f4a-f67b-4178-b58b-b3b4261e8d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_embeds:  torch.Size([1, 1, 4096])\n",
      "p_after_embeds torch.Size([1, 15, 4096])\n",
      "inputs_embeds torch.Size([1, 23, 4096])\n",
      "input_embeds torch.Size([1, 23, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The image shows a white wolf standing on its hind legs with its front paws resting on its chest. The wolf's ears are perked up and its eyes are focused on something in the distance. The wolf's mouth is slightly open, and its tongue is hanging out. The wolf appears to be alert and attentive, possibly reacting to a sound or smell in the environment.\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(path='./ImageBind/all_assets/wolves.wav', audio=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imagebind]",
   "language": "python",
   "name": "conda-env-imagebind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
