{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617fd10-cad1-4bc3-82c4-96c4d520c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import imagebind.data as data\n",
    "from IPython.display import Audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3da54e-b67d-4d41-a32f-3bb3437e9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3a495-d5b1-44fa-a423-ae45bd635817",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba319f-6906-4fdd-a91a-a80dd927f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975a769",
   "metadata": {},
   "source": [
    "#### Convert MEL to Wav\n",
    "This code is not accurate, i.e. wav->mel->wav does not really work\n",
    "well, but it has acceptable quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a743e-fd13-4a48-841b-1c99692f6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_mel_bins = 128\n",
    "num_frames = 204\n",
    "sample_rate = 16000\n",
    "n_fft = 400\n",
    "hop_length = n_fft//4\n",
    "win_length = n_fft\n",
    "\n",
    "# Function to create a Mel inversion matrix\n",
    "def create_mel_inversion_matrix(sr, n_fft, n_mels, fmin=0.0, fmax=None):\n",
    "    # Create a Mel filter bank using torchaudio\n",
    "    mel_fb = T.MelScale(n_mels, sr, f_min=fmin, f_max=fmax, n_stft=n_fft//2+1, norm=None)\n",
    "    # Convert the filter bank to a tensor\n",
    "    mel_fb_tensor = torch.tensor(mel_fb.fb, dtype=torch.float)\n",
    "    # Calculate the pseudo inverse\n",
    "    inversion_matrix = torch.pinverse(mel_fb_tensor)\n",
    "    print(inversion_matrix.shape)\n",
    "    \n",
    "    return inversion_matrix\n",
    "\n",
    "def inverse_it(mel_spectrogram):\n",
    "    \n",
    "    # Create the Mel inversion matrix\n",
    "    inversion_matrix = create_mel_inversion_matrix(sample_rate, n_fft, num_mel_bins)\n",
    "\n",
    "    # Invert the Mel spectrogram to a power spectrogram\n",
    "    power_spectrogram = torch.matmul(mel_spectrogram, inversion_matrix)\n",
    "\n",
    "    # Create an InverseMelScale transform\n",
    "    inverse_mel_scale_transform = T.InverseMelScale(\n",
    "        n_stft=n_fft//2+1,\n",
    "        n_mels=num_mel_bins,\n",
    "        sample_rate=sample_rate,\n",
    "        f_min=0.0,\n",
    "        f_max=sample_rate//2,\n",
    "        norm=None\n",
    "    )\n",
    "\n",
    "    # Apply the InverseMelScale transform to the Mel spectrogram\n",
    "    spectrogram = inverse_mel_scale_transform(mel_spectrogram.T)\n",
    "\n",
    "    # Initialize Griffin-Lim transform\n",
    "    griffin_lim = T.GriffinLim(n_fft=n_fft, n_iter=32, win_length=win_length, hop_length=hop_length)\n",
    "\n",
    "    # Recover the waveform from the spectrogram\n",
    "    recovered_waveform = griffin_lim(spectrogram)\n",
    "    \n",
    "    Audio(recovered_waveform, rate=16000)\n",
    "    \n",
    "    return recovered_waveform\n",
    "\n",
    "def inverse_normalize(melspec, mean=-4.268, std=9.138):\n",
    "    return melspec * std + mean\n",
    "\n",
    "\n",
    "def combine_results(audio):\n",
    "    results = list()\n",
    "    for i in range(audio.shape[1]):\n",
    "        res = inverse_it(inverse_normalize(audio.clone().detach().cpu().float()[0][i][0]).T)\n",
    "        results.append(res)\n",
    "    return [results[0], results[1], results[2]]\n",
    "    # return [results[0][:-5000], results[1][:-5000], results[2][:-2000]]\n",
    "    \n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def get_results(audio_tensor):\n",
    "    mel_spectrogram_np = audio_tensor[0, :, 0, :, :198].detach().clone().cpu().float()\n",
    "    audio_cropped = torch.cat(tuple(mel_spectrogram_np), dim=-1)\n",
    "    return inverse_it(inverse_normalize(audio_cropped.T))\n",
    "\n",
    "\n",
    "def convert_mp3_to_wav(input_path, output_path, bitrate, duration, shift=0):\n",
    "    # Load the MP3 file\n",
    "    audio = AudioSegment.from_mp3(input_path)\n",
    "    print(len(audio))\n",
    "    # Set the desired duration\n",
    "    audio = audio[shift:duration * 1000 + shift]\n",
    "\n",
    "    # Set the desired bitrate\n",
    "    audio = audio.set_frame_rate(bitrate)\n",
    "\n",
    "    # Export the audio as a WAV file\n",
    "    audio.export(output_path, format='wav')\n",
    "    return audio\n",
    "\n",
    "def plot_mel(spectrogram_tensor):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Assuming `spectrogram_tensor` is your tensor of shape torch.Size([1, 3, 1, 128, 204])\n",
    "    \n",
    "    # Convert PyTorch tensor to numpy array\n",
    "    spectrogram_np = spectrogram_tensor.numpy()\n",
    "    \n",
    "    # Squeeze the array to remove singleton dimensions and select one channel\n",
    "    spectrogram_np_squeezed = np.squeeze(spectrogram_np)[0]\n",
    "    \n",
    "    # Plotting the spectrogram\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(spectrogram_np_squeezed, aspect='auto', origin='lower')\n",
    "    plt.title('Mel spectrogram')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284c6c6-6710-484e-89ab-3f68ede65048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the input MP3 file path, output WAV file path, bitrate, and duration\n",
    "input_file = 'all_assets/YOUR_FILE.mp3'\n",
    "output_file = 'all_assets/YOUR_FILE.wav'\n",
    "bitrate = 16000  # 16 kHz\n",
    "duration = 10  # 10 seconds\n",
    "\n",
    "# Convert the MP3 to WAV\n",
    "audio = convert_mp3_to_wav(input_file, output_file, bitrate, duration, shift=000)\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6b567-5e3b-4fef-b574-27c6392b8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ImageBind/.assets/dog_audio.wav\"\n",
    "orig_waveform, sr = torchaudio.load(path)\n",
    "Audio(orig_waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d4e4f-32c6-4b5d-96b7-d90a0769fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mel(audio_tensor.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b766c-7b57-44fe-bb40-8c950fa29450",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=['A classical music concert',]\n",
    "# image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "# image_paths = ['all_assets/dragon.jpg',]\n",
    "# audio_paths=[\".assets/bird_audio.wav\"] # \".assets/car_audio.wav\", \".assets/bird_audio.wav\"\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    # ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embed = model.forward( {ModalityType.TEXT: data.load_and_transform_text(text_list, device)}, normalize=False)[ModalityType.TEXT] \n",
    "    # audio_embed = model.forward( {ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device)}, normalize=True)[ModalityType.AUDIO]\n",
    "    # image_embed = model.forward({ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device)}, normalize=False)[ModalityType.VISION]\n",
    "\n",
    "ideal_embed = text_embed.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303a15b-c05e-4062-b102-1575bb716fb8",
   "metadata": {},
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c4d56-030e-4b0b-a1e4-09b68ee6852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.0001 * torch.rand_like(audio_tensor).to(device)\n",
    "X.requires_grad_(True)\n",
    "audio_tensor = audio_tensor.to(device)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb70db-b9e8-42fe-b3ac-85d89109e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "optimizer = optim.SGD([X], lr=0.005)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                              T_max = epochs, # Maximum number of iterations.\n",
    "                              eta_min = 1e-5) # Minimum learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6865eb0-8983-44a8-901e-72b3a502669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(epochs))\n",
    "saved_dict = dict()\n",
    "\n",
    "for i in pbar:\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    embeds = model.forward({'audio': X + audio_tensor}, normalize=True)\n",
    "    \n",
    "    # use cosine similarity\n",
    "    loss = 1 - F.cosine_similarity(embeds['audio'], ideal_embed, dim=1).mean()\n",
    "    grads = torch.autograd.grad(outputs=loss, inputs=X)\n",
    "    \n",
    "    X = X - lr * grads[0].sign()\n",
    "\n",
    "    # clamp amount of perturbation\n",
    "    X.detach().clamp_(min=-0.05, max=0.05)\n",
    "\n",
    "    \n",
    "    pbar.set_postfix({'loss': loss.item(), 'lr': lr, 'norm': X.detach().norm().item(), 'saved': list(saved_dict.keys())})\n",
    "    scheduler.step()\n",
    "    del grads, embeds, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a941a8f-c27f-4e0b-b3ad-05507f4dccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((X + audio_tensor).detach().cpu(), f'hack/results.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04214c72-90e9-4cec-8a73-d4d5eb965c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mel((X + audio_tensor).detach().cpu()[:,:,:,:,:-6])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imagebind]",
   "language": "python",
   "name": "conda-env-imagebind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
